{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumTree import Memory\n",
    "from DQN import DQNAgent\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # Convert RGB to grayscale\n",
    "    if len(obs.shape) == 3 and obs.shape[-1] == 3:  # RGB input\n",
    "        obs = np.dot(obs[..., :3], [0.2989, 0.5870, 0.1140])  # Grayscale\n",
    "    # Normalize to [0, 1]\n",
    "    obs = obs / 255.0\n",
    "    # Flatten if necessary\n",
    "    obs = obs.flatten()\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     49\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 50\u001b[0m next_state, reward, done, trauncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# reward = reward if not done else -10\u001b[39;00m\n\u001b[0;32m     52\u001b[0m next_state \u001b[38;5;241m=\u001b[39m preprocess_observation(next_state)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    210\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    211\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:531\u001b[0m, in \u001b[0;36mBipedalWalker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mmotorSpeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(SPEED_KNEE \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action[\u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmotorSpeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(SPEED_HIP \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msign(action[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmaxMotorTorque \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[0;32m    533\u001b[0m         MOTORS_TORQUE \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mabs(action[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    534\u001b[0m     )\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmotorSpeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(SPEED_KNEE \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msign(action[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Create TensorBoard callback\n",
    "log_dir = \"logs/dqn_Bipedal\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# alada logs/dqn or Log/ddqn\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "checkpoint_path = \"checkpoints/dqn_Bipedal_checkpoint_{epoch:02d}.weights.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,  # Only save weights, not the full model\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',       # Save every epoch (you can change this)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_save_path = \"saved_models/dqn_model_Bipedal\"\n",
    "\n",
    "\n",
    "# Lists to store metrics for each episode\n",
    "episode_rewards = []\n",
    "episode_epsilons = []\n",
    "\n",
    "# to write the tensorboard logs\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"human\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scores = []\n",
    "EPISODES = 2000\n",
    "avg_window = 100\n",
    "\n",
    "save_interval = 100\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state, _ = env.reset(seed=42)\n",
    "    # print(f'state : {state}')\n",
    "    state = preprocess_observation(state)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    time = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        time += 1\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, trauncated, _ = env.step(action)\n",
    "        # reward = reward if not done else -10\n",
    "        next_state = preprocess_observation(next_state)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done or trauncated:\n",
    "            scores.append(total_reward)\n",
    "            if e % 100 == 0 and e > 1:\n",
    "                print(\"episode: {}/{}, Score Mean: {} / Median: {} \".format(e, EPISODES, int(np.mean(scores)), int(np.median(scores))))\n",
    "                print(\"Beta {:.5f} / Eps: {:.5f}\".format(agent.memory.beta, agent.epsilon))\n",
    "            # scores.append(time)\n",
    "    if agent.memory.tree.n_entries > 1000:\n",
    "        agent.replay()\n",
    "\n",
    "    # Save manually if desired or use replay\n",
    "    if e % save_interval == 0:\n",
    "        agent.model.save(f\"{model_save_path}_episode_{e}.h5\")\n",
    "\n",
    "    # Use the callback for automated saving\n",
    "    if e == 0:  # First save\n",
    "        checkpoint_callback.set_model(agent.model)\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Total Reward', total_reward, step=e)\n",
    "        tf.summary.scalar('Epsilon', agent.epsilon, step=e)\n",
    "\n",
    "        # Compute and log the average reward over the last 100 episodes\n",
    "        if len(scores) >= avg_window:\n",
    "            avg_reward = np.mean(scores[-avg_window:])\n",
    "            tf.summary.scalar('Average Reward (last 100 episodes)', avg_reward, step=e)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
